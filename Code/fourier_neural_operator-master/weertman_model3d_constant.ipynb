{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "os.environ['export OPENBLAS_NUM_THREADS']='2'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities3 import *\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "from Adam import Adam\n",
    "\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 3d fourier layers\n",
    "################################################################\n",
    "\n",
    "class SpectralConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2, modes3):\n",
    "        super(SpectralConv3d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        3D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "        self.modes3 = modes3\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights3 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights4 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul3d(self, input, weights):\n",
    "        # (batch, in_channel, x,y,t ), (in_channel, out_channel, x,y,t) -> (batch, out_channel, x,y,t)\n",
    "        return torch.einsum(\"bixyz,ioxyz->boxyz\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfftn(x, dim=[-3,-2,-1])\n",
    "        \n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-3), x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, :self.modes1, :self.modes2, :self.modes3], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, -self.modes1:, :self.modes2, :self.modes3], self.weights2)\n",
    "        out_ft[:, :, :self.modes1, -self.modes2:, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, :self.modes1, -self.modes2:, :self.modes3], self.weights3)\n",
    "        out_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3], self.weights4)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfftn(out_ft, s=(x.size(-3), x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO3d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, modes3, width):\n",
    "        super(FNO3d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t). It's a constant function in time, except for the last index.\n",
    "        input shape: (batchsize, x=64, y=64, t=40, c=13)\n",
    "        output: the solution of the next 40 timesteps\n",
    "        output shape: (batchsize, x=64, y=64, t=40, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.modes3 = modes3\n",
    "        self.width = width\n",
    "        self.padding = 6 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(13, self.width)\n",
    "        # self.fc0 = nn.Linear(8, self.width)\n",
    "        # input channel is 12: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t)\n",
    "\n",
    "        self.conv0 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv1 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv2 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv3 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.w0 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.bn0 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn1 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn2 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn3 = torch.nn.BatchNorm3d(self.width)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        x = F.pad(x, [0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 4, 1) # pad the domain if input is non-periodic\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y, size_z = shape[0], shape[1], shape[2], shape[3]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1, 1).repeat([batchsize, 1, size_y, size_z, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1, 1).repeat([batchsize, size_x, 1, size_z, 1])\n",
    "        gridz = torch.tensor(np.linspace(0, 1, size_z), dtype=torch.float)\n",
    "        gridz = gridz.reshape(1, 1, 1, size_z, 1).repeat([batchsize, size_x, size_y, 1, 1])\n",
    "        return torch.cat((gridx, gridy, gridz), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.001 100 0.5\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# configs\n",
    "################################################################\n",
    "DATA_PATH = 'Solutions/acc_rate_initial_constant.npy'\n",
    "\n",
    "# currently data are 370 samples\n",
    "ntrain = 300\n",
    "ntest = 70\n",
    "\n",
    "modes = 8\n",
    "width = 20\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "epochs = 100 #500\n",
    "learning_rate = 0.001\n",
    "scheduler_step = 100\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "print(epochs, learning_rate, scheduler_step, scheduler_gamma)\n",
    "\n",
    "path = 'acc_rate3d_constant_ep500_Tin1'\n",
    "path_model = 'model/'+path\n",
    "path_train_err = 'results/'+path+'train.txt'\n",
    "path_test_err = 'results/'+path+'test.txt'\n",
    "path_image = 'image/'+path\n",
    "\n",
    "runtime = np.zeros(2, )\n",
    "t1 = default_timer()\n",
    "\n",
    "S1 = 65\n",
    "S2 = 97\n",
    "T_in = 10\n",
    "T = 10\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 65, 97, 10])\n",
      "torch.Size([70, 65, 97, 10])\n",
      "preprocessing finished, time used: 0.5126456599973608\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# load data\n",
    "################################################################\n",
    "data_gen = np.load(DATA_PATH)\n",
    "\n",
    "train_a = torch.tensor(data_gen[:ntrain,:,:,:T_in], dtype=torch.float)\n",
    "train_u = torch.tensor(data_gen[:ntrain,:,:,T_in:T+T_in], dtype=torch.float)\n",
    "\n",
    "test_a = torch.tensor(data_gen[-ntest:,:,:,:T_in], dtype=torch.float)\n",
    "test_u = torch.tensor(data_gen[-ntest:,:,:,T_in:T+T_in], dtype=torch.float)\n",
    "\n",
    "print(train_u.shape)\n",
    "print(test_a.shape)\n",
    "assert (S1 == train_u.shape[-3])\n",
    "assert (S2 == train_u.shape[-2])\n",
    "assert (T == train_u.shape[-1])\n",
    "\n",
    "\n",
    "a_normalizer = UnitGaussianNormalizer(train_a)\n",
    "with open('a_normalizer_constant.pkl', 'wb') as f:\n",
    "    pickle.dump(a_normalizer, f)\n",
    "\n",
    "train_a = a_normalizer.encode(train_a)\n",
    "test_a = a_normalizer.encode(test_a)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(train_u)\n",
    "with open('y_normalizer_constant.pkl', 'wb') as f:\n",
    "    pickle.dump(y_normalizer, f)\n",
    "train_u = y_normalizer.encode(train_u)\n",
    "\n",
    "train_a =  train_a.reshape(ntrain,S1,S2,1,T_in).repeat([1,1,1,T,1])\n",
    "test_a = test_a.reshape(ntest,S1,S2,1,T_in).repeat([1,1,1,T,1])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "t2 = default_timer()\n",
    "\n",
    "print('preprocessing finished, time used:', t2-t1)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6558537\n",
      "0 2.390055518000736 1.0105550050735475 0.2824845862388611 0.41631597450801305\n",
      "1 2.3734334479959216 0.37999996592601143 0.1226729999979337 0.0609959534236363\n",
      "2 2.3734270589920925 0.05032013282179833 0.03531725501020749 0.013403576399598802\n",
      "3 2.3777048680058215 0.04417330486079057 0.02195672002931436 0.010307077052337783\n",
      "4 2.3781967779941624 0.03821157334993283 0.008019686502714951 0.010417680814862251\n",
      "5 2.3784668689913815 0.038026850391179326 0.005874524340033532 0.008810733790908541\n",
      "6 2.377216007997049 0.03893498343725999 0.01022142868489027 0.007304637293730464\n",
      "7 2.3790640279912623 0.038784720748662946 0.008429095049699147 0.010897845242704663\n",
      "8 2.379169837993686 0.03821921348571777 0.005127705441166957 0.006698771406497274\n",
      "9 2.3789898690010887 0.0383935176456968 0.005930116275946299 0.006689929483192307\n",
      "10 2.382401088005281 0.039071280198792614 0.008288837286333244 0.01872959498848234\n",
      "11 2.3880387070094002 0.03882716000080109 0.0069568954532345136 0.014267427261386599\n",
      "12 2.3845407079934375 0.039178043231368066 0.008946552524964015 0.015747727560145513\n",
      "13 2.3853504879953107 0.03844144716858864 0.004923144212613503 0.012138810115201133\n",
      "14 2.3856426979909884 0.03845914223541816 0.004731826726347208 0.005834776535630226\n",
      "15 2.385246078003547 0.03858798717459043 0.0059145670011639595 0.01621065522943224\n",
      "16 2.3868665670015616 0.03897976782172918 0.0057039966310064 0.01956365065915244\n",
      "17 2.4012330469995504 0.03842077873026331 0.004390159286558628 0.008283223424639021\n",
      "18 2.399149937002221 0.038352353995045024 0.0038205737248063086 0.012801952553646904\n",
      "19 2.3993980170052964 0.03851550451169412 0.004490908284982046 0.008509862742253712\n",
      "20 2.3990518570062704 0.03886893776555856 0.006132294703274965 0.019972291375909532\n",
      "21 2.400166696999804 0.03875374579802156 0.00676839272181193 0.018204001656600407\n",
      "22 2.505257311000605 0.03869305526216825 0.007032696676130096 0.0057006223393338065\n",
      "23 2.3999707070033764 0.038406111672520636 0.0043714971467852595 0.007521259358951024\n",
      "24 2.3991740670026047 0.03844194629540046 0.0046498258815457424 0.008952344102518899\n",
      "25 2.4022581260069273 0.03824414368718863 0.0038972293213009835 0.007087323974285807\n",
      "26 2.4063118570047664 0.03844513387108842 0.004548519390324752 0.00543845822768552\n",
      "27 2.402293577004457 0.03829825147986412 0.003465467778344949 0.010948738242898668\n",
      "28 2.415501415991457 0.03828160914902886 0.004493403378874063 0.007169314952833312\n",
      "29 2.400302077003289 0.03850641089181105 0.00577593211705486 0.008167602600795881\n",
      "30 2.401468786993064 0.038669515897830326 0.0065281356684863566 0.006779725370662553\n",
      "31 2.4000054569914937 0.03832141390691201 0.004786243208994468 0.004620811103710106\n",
      "32 2.4035241559904534 0.03837279124806325 0.005349684488028288 0.014562221084322249\n",
      "33 2.4040753570006927 0.038465048931539056 0.005956770932922761 0.008870736669216837\n",
      "34 2.4003253370028688 0.03813372602065404 0.004476816902558009 0.0049781205930880135\n",
      "35 2.4054621670074994 0.03807059482981761 0.003345100755492846 0.005961482599377632\n",
      "36 2.4019256360043073 0.038104411028325555 0.004184113014489412 0.008883403294852802\n",
      "37 2.405183976996341 0.038357227481901644 0.006779821403324604 0.015042800243411746\n",
      "38 2.4058041170064826 0.038597695901989935 0.00662608103826642 0.009937790674822672\n",
      "39 2.3981470169965178 0.038520078733563425 0.007545576567451159 0.01630828572171075\n",
      "40 2.4073985959985293 0.03792570897688468 0.0033239705736438432 0.005132110283843108\n",
      "41 2.4074858170060907 0.03807236524298787 0.00488592652293543 0.005398486235312053\n",
      "42 2.401355316993431 0.038193543317417304 0.005805620787044366 0.015378737662519728\n",
      "43 2.40642819700588 0.03794302344322205 0.004074873967717091 0.005269848023142134\n",
      "44 2.404413137002848 0.038084458808104196 0.004952733448396127 0.010167332632201058\n",
      "45 2.4093497959984234 0.037867971292386456 0.004150238906343778 0.005072163444544588\n",
      "46 2.4012932170007844 0.03802056511243184 0.004737496146311362 0.004530403709837368\n",
      "47 2.39381360700645 0.037836574887235956 0.003964733015745878 0.013515240379742213\n",
      "48 2.3863151879922953 0.03815110394110282 0.004816695898771286 0.004665815085172653\n",
      "49 2.4051707970065763 0.0383519446477294 0.007183264972021182 0.008799642963068826\n",
      "50 2.50792819999333 0.038196235646804175 0.007023736207435529 0.005602807072656495\n",
      "51 2.397208407011931 0.03797696962331732 0.0053875064104795456 0.011947429499455861\n",
      "52 2.397532066999702 0.03811704075584809 0.006135169410457214 0.008669017042432512\n",
      "53 2.4001891969965072 0.03780429723362128 0.0039799960764745875 0.004557885282805988\n",
      "54 2.4012943470006576 0.03793869124104579 0.005347848714639743 0.004365539364516735\n",
      "55 2.4027406070090365 0.0379987974340717 0.006425820812582969 0.004110819793173245\n",
      "56 2.400205236990587 0.03787946732093891 0.0051937985544403395 0.00963319508092744\n",
      "57 2.3999286869948264 0.03810913606236378 0.007499748573948939 0.01686795481613704\n",
      "58 2.396374467003625 0.038652786922951536 0.01097947932779789 0.00433065223374537\n",
      "59 2.402427577006165 0.03786956261222561 0.0053197164833545686 0.009539543198687689\n",
      "60 2.4008775670081377 0.03793090721592307 0.006265702744325002 0.01177047576223101\n",
      "61 2.398719666991383 0.038502743219335875 0.008123139074693123 0.01430867612361908\n",
      "62 2.4030106760037597 0.037574011459946634 0.003743702657520771 0.00802714079618454\n",
      "63 2.3996603670093464 0.038136810498933 0.00787300664310654 0.004576580918260983\n",
      "64 2.397073987987824 0.037961285250882305 0.006940131566176812 0.010083266134772981\n",
      "65 2.39824620699801 0.03797449593742688 0.006196717986216148 0.007396096789411136\n",
      "66 2.3974160570069216 0.0383458146204551 0.008489158898591996 0.01659892329147884\n",
      "67 2.395875326998066 0.03746425422529379 0.0032504028640687466 0.005947693863085338\n",
      "68 2.397060146991862 0.03747742846608162 0.0031990401819348337 0.004339697398245334\n",
      "69 2.394354097006726 0.037423751006523766 0.0034555497268835703 0.00624514248754297\n",
      "70 2.394531227997504 0.03747309527049462 0.0036645318878193696 0.004815204707639558\n",
      "71 2.394932416995289 0.03767565904806058 0.005166316740214825 0.004317047686449119\n",
      "72 2.3932977370131994 0.03816726300865412 0.00845636824766795 0.004671852343848773\n",
      "73 2.397230126996874 0.03751216890911261 0.004406014550477266 0.008242770018322127\n",
      "74 2.3951075469958596 0.03761312322070201 0.005933209533492724 0.004890090573046888\n",
      "75 2.394183867989341 0.03735857183734576 0.004461474524190028 0.013474403321743012\n",
      "76 2.3961948569922242 0.037373306105534236 0.004067702939112981 0.004599264583417348\n",
      "77 2.3983946869993815 0.03727040986220042 0.0030595505703240634 0.005915860459208488\n",
      "78 2.5098677499918267 0.03724557763586442 0.003915054152409236 0.006505271685974938\n",
      "79 2.394350528003997 0.037630645620326204 0.006621597306802869 0.018487840678010667\n",
      "80 2.3966217970009893 0.03743622535839677 0.004393171376238267 0.008543382052864347\n",
      "81 2.3966812169965124 0.03717795837049683 0.0033048743506272635 0.006913614592381886\n",
      "82 2.392314726996119 0.037930469525357086 0.008339285850524902 0.0057694066049797195\n",
      "83 2.394155538000632 0.037276494596153495 0.004828545258690913 0.004533024850700583\n",
      "84 2.393948996992549 0.03723600742717584 0.0042061972245574 0.008649897841470583\n",
      "85 2.4326058650040068 0.03744272803887725 0.005993963070213795 0.01338721662759781\n",
      "86 2.4284801649919245 0.037574849805484214 0.006214485832800468 0.00670325117451804\n",
      "87 2.4034990569925867 0.0371121329565843 0.0028721433132886886 0.007809719177229064\n",
      "88 2.3911483970005065 0.037195771994690104 0.003792565936843554 0.005872388396944319\n",
      "89 2.392228458003956 0.037182651770611606 0.0043631030122439065 0.008704800265175956\n",
      "90 2.394726066995645 0.037345768262942634 0.005622660319010417 0.0061525134635823115\n",
      "91 2.394685657010996 0.03720426984752218 0.0043572764253864684 0.007985490241221018\n",
      "92 2.3916281679994427 0.03713626315196355 0.00425225464006265 0.005012661005769457\n",
      "93 2.393312526997761 0.037058518702785176 0.0028491635931034882 0.00740405981029783\n",
      "94 2.393702507004491 0.03723431670417388 0.0044107032753527165 0.003826368467083999\n",
      "95 2.38890313799493 0.037676925957202914 0.007924483927587668 0.008330707039151872\n",
      "96 2.3958501070010243 0.03713972208400567 0.00441653177763025 0.006801259411232812\n",
      "97 2.3921294979954837 0.03708430429299672 0.004025972144057353 0.004760548365967615\n",
      "98 2.396452696993947 0.03740429114550352 0.006327834986150265 0.00411585122346878\n",
      "99 2.3956430869875476 0.03710613970955213 0.004458923386409879 0.009877937180655343\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "model = FNO3d(modes, modes, modes, width).cuda()\n",
    "\n",
    "print(count_params(model))\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda()\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x).view(batch_size, S1, S2, T)\n",
    "        \n",
    "        mse = F.mse_loss(out, y, reduction='mean')\n",
    "        # mse.backward()\n",
    "\n",
    "        y = y_normalizer.decode(y)\n",
    "        out = y_normalizer.decode(out)\n",
    "        l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "        l2.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_mse += mse.item()\n",
    "        train_l2 += l2.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            out = model(x).view(batch_size, S1, S2, T)\n",
    "            out = y_normalizer.decode(out)\n",
    "            test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "\n",
    "    train_mse /= len(train_loader)\n",
    "    train_l2 /= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_mse, train_l2, test_l2)\n",
    "# torch.save(model, path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.018128719180822372\n",
      "0 0.018193913623690605\n",
      "0 0.018262118101119995\n",
      "0 0.018333857879042625\n",
      "0 0.018414774909615517\n",
      "0 0.018504535779356956\n",
      "0 0.018602078780531883\n",
      "0 0.01870839111506939\n",
      "0 0.01882297918200493\n",
      "0 0.018945304676890373\n",
      "0 0.01907384768128395\n",
      "0 0.019194385036826134\n",
      "0 0.019456975162029266\n",
      "0 0.01974499225616455\n",
      "0 0.02003481797873974\n",
      "0 0.02032717876136303\n",
      "0 0.020612208172678947\n",
      "0 0.020895246416330338\n",
      "0 0.021173207089304924\n",
      "0 0.02143574133515358\n",
      "0 0.021675173193216324\n",
      "0 0.02190515026450157\n",
      "0 0.022128963842988014\n",
      "0 0.0223232414573431\n",
      "0 0.022487416863441467\n",
      "0 0.02264459803700447\n",
      "0 0.022777611389756203\n",
      "0 0.022883661091327667\n",
      "0 0.02299516461789608\n",
      "0 0.023112352937459946\n",
      "0 0.02323441207408905\n",
      "0 0.023354798555374146\n",
      "0 0.023476962000131607\n",
      "0 0.02360466495156288\n",
      "0 0.023737527430057526\n",
      "0 0.023871874436736107\n",
      "0 0.023997390642762184\n",
      "0 0.024119224399328232\n",
      "0 0.024244220927357674\n",
      "0 0.024373725056648254\n",
      "0 0.024640394374728203\n",
      "0 0.0251249298453331\n",
      "0 0.025620348751544952\n",
      "0 0.02610672637820244\n",
      "0 0.026575405150651932\n",
      "0 0.027035454288125038\n",
      "0 0.027480702847242355\n",
      "0 0.02789558656513691\n",
      "0 0.02826324850320816\n",
      "0 0.028606681153178215\n",
      "0 0.028914665803313255\n",
      "0 0.02918343059718609\n",
      "0 0.02938862331211567\n",
      "0 0.029566405341029167\n",
      "0 0.02973547764122486\n",
      "0 0.02981961891055107\n",
      "0 0.029897058382630348\n",
      "0 0.02998017705976963\n",
      "0 0.030064918100833893\n",
      "0 0.030155513435602188\n",
      "0 0.03025198169052601\n",
      "0 0.0303526371717453\n",
      "0 0.0304564256221056\n",
      "0 0.030565986409783363\n",
      "0 0.03068104013800621\n",
      "0 0.03080112114548683\n",
      "0 0.03093036822974682\n",
      "0 0.03118649497628212\n",
      "0 0.031477391719818115\n",
      "0 0.03177077695727348\n"
     ]
    }
   ],
   "source": [
    "pred = torch.zeros(test_u.shape)\n",
    "\n",
    "index = 0\n",
    "# model = torch.load(\"model/acc_rate3d_constant_ep100_Tin10\")\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=1, shuffle=False)\n",
    "first_output = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        test_l2 = 0\n",
    "        if index == 0:\n",
    "            first_output = y.clone()\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        out = model(x)\n",
    "        out = y_normalizer.decode(out[:,:,:,0])\n",
    "        pred[index] = out\n",
    "\n",
    "        test_l2 += myloss(out.view(1, -1), y.view(1, -1)).item()\n",
    "        print(index, test_l2)\n",
    "\n",
    "# for i in range(18):\n",
    "#     cp = plt.imshow(abs(pred[0,:,:,i]-first_output[0,:,:,i]))\n",
    "#     plt.colorbar(cp)\n",
    "#     plt.show()\n",
    "#     print(i)\n",
    "    # cp = plt.imshow(first_output[0,:,:,i])\n",
    "    # plt.colorbar(cp)\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f8c49416430bf6f9356715c0a0173afd7466c7f6261729e3b70b73af4f7e4ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
