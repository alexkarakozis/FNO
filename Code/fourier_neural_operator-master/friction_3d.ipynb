{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "os.environ['export OPENBLAS_NUM_THREADS']='2'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities3 import *\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "from Adam import Adam\n",
    "\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 3d fourier layers\n",
    "################################################################\n",
    "\n",
    "class SpectralConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2, modes3):\n",
    "        super(SpectralConv3d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        3D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "        self.modes3 = modes3\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights3 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights4 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul3d(self, input, weights):\n",
    "        # (batch, in_channel, x,y,t ), (in_channel, out_channel, x,y,t) -> (batch, out_channel, x,y,t)\n",
    "        return torch.einsum(\"bixyz,ioxyz->boxyz\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfftn(x, dim=[-3,-2,-1])\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-3), x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, :self.modes1, :self.modes2, :self.modes3], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, -self.modes1:, :self.modes2, :self.modes3], self.weights2)\n",
    "        out_ft[:, :, :self.modes1, -self.modes2:, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, :self.modes1, -self.modes2:, :self.modes3], self.weights3)\n",
    "        out_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3], self.weights4)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfftn(out_ft, s=(x.size(-3), x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO3d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, modes3, width):\n",
    "        super(FNO3d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t). It's a constant function in time, except for the last index.\n",
    "        input shape: (batchsize, x=64, y=64, t=40, c=13)\n",
    "        output: the solution of the next 40 timesteps\n",
    "        output shape: (batchsize, x=64, y=64, t=40, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.modes3 = modes3\n",
    "        self.width = width\n",
    "        self.padding = 15 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(4, self.width)\n",
    "        # input channel is 12: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t)\n",
    "\n",
    "        self.conv0 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv1 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv2 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv3 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.w0 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.bn0 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn1 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn2 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn3 = torch.nn.BatchNorm3d(self.width)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        x = F.pad(x, [0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 4, 1) # pad the domain if input is non-periodic\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y, size_z = shape[0], shape[1], shape[2], shape[3]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1, 1).repeat([batchsize, 1, size_y, size_z, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1, 1).repeat([batchsize, size_x, 1, size_z, 1])\n",
    "        gridz = torch.tensor(np.linspace(0, 1, size_z), dtype=torch.float)\n",
    "        gridz = gridz.reshape(1, 1, 1, size_z, 1).repeat([batchsize, size_x, size_y, 1, 1])\n",
    "        return torch.cat((gridx, gridy, gridz), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.001 100 0.5\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# configs\n",
    "################################################################\n",
    "DATA_PATH = 'inverse/friction.npy'\n",
    "\n",
    "# currently data are 100 samples\n",
    "ntrain = 80\n",
    "ntest = 20\n",
    "\n",
    "modes = 8\n",
    "width = 20\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "epochs = 500\n",
    "learning_rate = 0.001\n",
    "scheduler_step = 100\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "print(epochs, learning_rate, scheduler_step, scheduler_gamma)\n",
    "\n",
    "path = f'friction_ep{epochs}'\n",
    "path_model = 'model/'+path\n",
    "path_train_err = 'results/'+path+'train.txt'\n",
    "path_test_err = 'results/'+path+'test.txt'\n",
    "path_image = 'image/'+path\n",
    "\n",
    "runtime = np.zeros(2, )\n",
    "t1 = default_timer()\n",
    "\n",
    "S1 = 65\n",
    "S2 = 97\n",
    "T_in = 1\n",
    "T = 1\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 65, 97, 1])\n",
      "torch.Size([20, 65, 97, 1])\n",
      "preprocessing finished, time used: 4.108067384979222\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# load data\n",
    "################################################################\n",
    "data_gen = np.load(DATA_PATH)\n",
    "np.random.shuffle(data_gen)\n",
    "train_a = torch.tensor(data_gen[:ntrain,:,:,:T_in], dtype=torch.float)\n",
    "train_u = torch.tensor(data_gen[:ntrain,:,:,T_in:T+T_in], dtype=torch.float)\n",
    "\n",
    "test_a = torch.tensor(data_gen[-ntest:,:,:,:T_in], dtype=torch.float)\n",
    "test_u = torch.tensor(data_gen[-ntest:,:,:,T_in:T+T_in], dtype=torch.float)\n",
    "\n",
    "print(train_a.shape)\n",
    "print(test_u.shape)\n",
    "assert (S1 == train_u.shape[-3])\n",
    "assert (S2 == train_u.shape[-2])\n",
    "assert (T == train_u.shape[-1])\n",
    "\n",
    "\n",
    "a_normalizer = UnitGaussianNormalizer(train_a)\n",
    "with open('a_normalizer_friction.pkl', 'wb') as f:\n",
    "    pickle.dump(a_normalizer, f)\n",
    "\n",
    "train_a = a_normalizer.encode(train_a)\n",
    "test_a = a_normalizer.encode(test_a)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(train_u)\n",
    "with open('y_normalizer_friction.pkl', 'wb') as f:\n",
    "    pickle.dump(y_normalizer, f)\n",
    "\n",
    "train_u = y_normalizer.encode(train_u)\n",
    "\n",
    "train_a = train_a.reshape(ntrain,S1,S2,1,T_in).repeat([1,1,1,T,1])\n",
    "test_a = test_a.reshape(ntest,S1,S2,1,T_in).repeat([1,1,1,T,1])\n",
    "\n",
    "# train_a = train_a.reshape(ntrain,S1,S2,T_in)\n",
    "# test_a = test_a.reshape(ntest,S1,S2,T_in)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "t2 = default_timer()\n",
    "\n",
    "print('preprocessing finished, time used:', t2-t1)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6558357\n",
      "0 1.7681465539499186 0.9787899777293205 0.07552469074726105 0.062369704246520996\n",
      "1 0.5293788390117697 0.9697020724415779 0.07383562959730625 0.059156543016433714\n",
      "2 0.528240397979971 0.9338831529021263 0.06980353519320488 0.05314151346683502\n",
      "3 0.5280200290144421 0.7827095463871956 0.05771316960453987 0.034380479156970976\n",
      "4 0.5280265780165792 0.4501522518694401 0.03939127288758755 0.016548773646354674\n",
      "5 0.5281496290117502 0.06964577082544565 0.015361623745411635 0.013905296474695206\n",
      "6 0.5280321380123496 0.03816987597383559 0.01043536989018321 0.005494110286235809\n",
      "7 0.5281920590205118 0.019254632003139704 0.005721521377563476 0.004291435703635216\n",
      "8 0.5284678679890931 0.01514988369308412 0.004753340315073728 0.0033349694684147837\n",
      "9 0.5282854689867236 0.0144187540281564 0.004479804076254368 0.0032810699194669724\n",
      "10 0.5283214879455045 0.014553058193996549 0.00447495486587286 0.003490307927131653\n",
      "11 0.5284089189954102 0.013396146008744836 0.0042894559912383555 0.00344168059527874\n",
      "12 0.5290027179871686 0.014806236955337226 0.004773664986714721 0.003887447901070118\n",
      "13 0.5305119790136814 0.013590321061201394 0.004212011815980077 0.003969363868236542\n",
      "14 0.5290559880086221 0.01503153855446726 0.004586961539462209 0.0029238397255539894\n",
      "15 0.5287896189838648 0.013739818998146802 0.0043088359292596575 0.0028216724283993246\n",
      "16 0.5295829080278054 0.012671214994043112 0.004144329391419887 0.003923790529370308\n",
      "17 0.5294597779866308 0.013548121671192348 0.004186748503707349 0.003346239123493433\n",
      "18 0.5292356089921668 0.012571007653605193 0.003893173020333052 0.0029890060424804686\n",
      "19 0.5288666479755193 0.013432179461233318 0.004276034096255899 0.0034601278603076935\n",
      "20 0.5293976189568639 0.014337530243210495 0.004597395937889814 0.0034428035840392114\n",
      "21 0.5288582079811022 0.01561132213100791 0.004688315466046334 0.003728591650724411\n",
      "22 0.5294994889991358 0.01512123120483011 0.004860631003975868 0.00301560964435339\n",
      "23 0.529020328016486 0.014352499507367611 0.004784855898469687 0.0034444056451320647\n",
      "24 0.5295001190388575 0.01591549685690552 0.005229108734056354 0.006185741350054741\n",
      "25 0.5292176879593171 0.014999368344433606 0.005777532886713743 0.0036742646247148515\n",
      "26 0.5296771680004895 0.013978161034174263 0.004539810121059418 0.00487456526607275\n",
      "27 0.5292824190109968 0.014167414337862283 0.0045440006069839 0.0033186040818691254\n",
      "28 0.5294640579959378 0.016004758654162288 0.005104945180937648 0.004670177586376667\n",
      "29 0.5296563489828259 0.013291273615323007 0.004132750770077109 0.0044498035684227945\n",
      "30 0.5296406479901634 0.012820079689845443 0.004402037896215916 0.00344178956001997\n",
      "31 0.5297640790231526 0.014442938612774014 0.00452879760414362 0.00425995122641325\n",
      "32 0.5300931880483404 0.012758591212332249 0.0039284151745960115 0.0037311546504497526\n",
      "33 0.5299766079988331 0.013341476791538298 0.004467187379486859 0.0040740640833973885\n",
      "34 0.530106769001577 0.012519863550551236 0.004059357079677283 0.0038136979565024376\n",
      "35 0.5298204179853201 0.013200383284129202 0.003969242237508297 0.0030700871720910072\n",
      "36 0.5299393989844248 0.012070098367985338 0.0035363076720386745 0.003013923391699791\n",
      "37 0.5299249279778451 0.013131604355294257 0.0041240222286432985 0.002646155096590519\n",
      "38 0.5307181579992175 0.012832944630645216 0.004050484485924244 0.005039159394800663\n",
      "39 0.5300608690013178 0.012944040005095303 0.004071991657838225 0.002582933846861124\n",
      "40 0.5308004079852253 0.012882481561973691 0.00393896868918091 0.0022748297080397604\n",
      "41 0.5303695089532994 0.011747500393539667 0.003164877975359559 0.002509842161089182\n",
      "42 0.5310715880477801 0.011558301339391619 0.0032288218615576627 0.0024119837209582327\n",
      "43 0.5309088879730552 0.013615897274576128 0.004258539108559489 0.00266643650829792\n",
      "44 0.5311554790241644 0.011917105759494007 0.003447354701347649 0.0030360667034983633\n",
      "45 0.5310575379990041 0.01225469191558659 0.0034933797782287002 0.0034333353862166403\n",
      "46 0.5309703579987399 0.01221085904398933 0.003673407342284918 0.003083673492074013\n",
      "47 0.5312682189978659 0.01278718130197376 0.003991249855607748 0.002736406959593296\n",
      "48 0.5306299380026758 0.013263250526506454 0.004155599419027567 0.0021225573495030404\n",
      "49 0.5315630480181426 0.013308731431607157 0.0038343995111063123 0.0032916711643338203\n",
      "50 0.5311081689433195 0.012250934436451644 0.003612580941990018 0.0028072468005120753\n",
      "51 0.5310863580089062 0.011842733307275921 0.0036674288101494314 0.002842526230961084\n",
      "52 0.5308663779869676 0.011596109834499657 0.00329908540006727 0.002193165197968483\n",
      "53 0.5309338890365325 0.011510142358019948 0.003091878048144281 0.0021536651998758315\n",
      "54 0.5311052079778165 0.011337959324009717 0.0030175494961440564 0.0023126730695366858\n",
      "55 0.5308679880108684 0.011336141848005354 0.00314087113365531 0.0021264243870973585\n",
      "56 0.5312087190104648 0.01146088377572596 0.0033440875820815565 0.0023833814077079294\n",
      "57 0.5307830480160192 0.011434567626565695 0.003205097489990294 0.002528241742402315\n",
      "58 0.5314304680214263 0.01173953094985336 0.003457105252891779 0.0031301564536988734\n",
      "59 0.5313389190123416 0.01131556072505191 0.0031258165370672943 0.002742351498454809\n",
      "60 0.53109680803027 0.011620194069109857 0.003485455410555005 0.0038995010778307914\n",
      "61 0.531585188000463 0.011206368857529014 0.0031443395651876926 0.0023032964207232\n",
      "62 0.5344912889995612 0.01105566683690995 0.00288076710421592 0.0024892514571547507\n",
      "63 0.5326710680383258 0.011308164685033262 0.003084005252458155 0.0033711440861225127\n",
      "64 0.5328973979922011 0.011235069832764566 0.0030522235902026296 0.00238303542137146\n",
      "65 0.5318955979892053 0.011095010908320546 0.002900740853510797 0.002249287813901901\n",
      "66 0.532649509026669 0.011207437608391047 0.0028576609445735814 0.0024331341497600078\n",
      "67 0.5323272480163723 0.011199283530004323 0.0030628312146291138 0.0025219096802175044\n",
      "68 0.5324123179889284 0.011105535028036684 0.003007141454145312 0.0024479065090417863\n",
      "69 0.534556217957288 0.011251855263253674 0.0030946780228987337 0.002066164743155241\n",
      "70 0.5330343579989858 0.011619658209383488 0.003159629972651601 0.0018907750956714154\n",
      "71 0.5333717790199444 0.011275158263742924 0.0028177935164421797 0.0024370569735765457\n",
      "72 0.5342005080310628 0.011248536873608828 0.0030941842356696724 0.003722902201116085\n",
      "73 0.5331800780259073 0.011051951732952148 0.0029092568205669523 0.0028015410527586935\n",
      "74 0.5336217679432593 0.011334326642099768 0.003190021659247577 0.002475253399461508\n",
      "75 0.5331770379561931 0.01220881927292794 0.003586940001696348 0.002649483270943165\n",
      "76 0.5333956590038724 0.01299991668201983 0.004086976777762175 0.004758062213659287\n",
      "77 0.532963857986033 0.013496795378159732 0.004067077836953104 0.002596488781273365\n",
      "78 0.5342738879844546 0.011997552821412683 0.0034413533052429558 0.0022468969225883486\n",
      "79 0.5336481180274859 0.013493540231138468 0.004033836582675576 0.003371577709913254\n",
      "80 0.5341512479935773 0.013545573456212878 0.004296110849827528 0.002632719185203314\n",
      "81 0.5345436579664238 0.011376629176083952 0.0030743448296561837 0.0023109659552574156\n",
      "82 0.5342479689861648 0.011645606893580407 0.0032490538200363518 0.0020688164979219435\n",
      "83 0.5347622280241922 0.011322036967612803 0.00312108329962939 0.0024463415145874025\n",
      "84 0.5343857780098915 0.01124627550598234 0.0030392227228730915 0.0021612994372844697\n",
      "85 0.53486088803038 0.011227476876229048 0.002953133312985301 0.0018156114965677261\n",
      "86 0.5344099180074409 0.011414031032472849 0.003020478645339608 0.0024514814838767053\n",
      "87 0.5341490979772061 0.011245043016970158 0.0031407540431246162 0.003119578957557678\n",
      "88 0.5332395990262739 0.011083908029831946 0.0029226361541077493 0.0021757006645202635\n",
      "89 0.5338717080303468 0.011106190620921552 0.0028545610373839738 0.0022127427160739898\n",
      "90 0.533897468005307 0.010995457938406616 0.0026498813414946197 0.00202074833214283\n",
      "91 0.5335653480142355 0.010948141571134329 0.0026938933413475754 0.0020351951941847803\n",
      "92 0.5339766479446553 0.011189641780219972 0.003028036607429385 0.003148957900702953\n",
      "93 0.5338884780067019 0.011523955501616001 0.003524050209671259 0.003373878076672554\n",
      "94 0.534622939012479 0.011494214530102909 0.0034728895872831343 0.0024218889884650707\n",
      "95 0.5339852680335753 0.011912721616681665 0.0033220809418708086 0.0021427396684885027\n",
      "96 0.5341761679737829 0.011662081873510033 0.003275860194116831 0.0033295071683824063\n",
      "97 0.5349764979910105 0.0112027344875969 0.003054836275987327 0.0025201458483934402\n",
      "98 0.5341159580275416 0.011041599733289331 0.0027963819447904825 0.0019739952869713305\n",
      "99 0.5339096080278978 0.011647001083474606 0.0031342795118689537 0.0026333734393119814\n",
      "100 0.5341249779448844 0.011392269167117774 0.003043550904840231 0.002056556846946478\n",
      "101 0.5341793589759618 0.010949845134746283 0.0026549439411610366 0.0018491793423891067\n",
      "102 0.5335435380111448 0.010769246611744165 0.0024751887656748297 0.002252966817468405\n",
      "103 0.5338335980195552 0.010867989039979875 0.002525670942850411 0.0018199310638010503\n",
      "104 0.5347521879593842 0.010862242605071515 0.0025111583760008215 0.0018373879604041577\n",
      "105 0.5342557479743846 0.011080170806962997 0.002696935087442398 0.0018663444556295873\n",
      "106 0.5336664780043066 0.010840440867468715 0.002539877011440694 0.001962711475789547\n",
      "107 0.5350578690413386 0.010865066957194358 0.0025381398852914573 0.001935467030853033\n",
      "108 0.5355628179968335 0.010778736788779497 0.0024769376730546357 0.0018178235739469528\n",
      "109 0.5340078780427575 0.010755203489679843 0.002577894600108266 0.0021748073399066926\n",
      "110 0.533932157966774 0.011001271952409297 0.00276171350851655 0.002242338377982378\n",
      "111 0.5346764880232513 0.011134831525851041 0.002813968877308071 0.002219436410814524\n",
      "112 0.5352655579918064 0.01101919315988198 0.0027961991261690857 0.0019446210004389285\n",
      "113 0.5334266779827885 0.01080642972374335 0.0025634860852733255 0.001876945234835148\n",
      "114 0.5337664089747705 0.010927407070994377 0.002723995316773653 0.0018083502538502217\n",
      "115 0.5357172579970211 0.011151749698910862 0.002870758809149265 0.0017517907544970512\n",
      "116 0.5345480979885906 0.010888646706007421 0.0026295487536117435 0.00195755111053586\n",
      "117 0.5348905680002645 0.01073148543946445 0.0025391407078132035 0.0020171464420855045\n",
      "118 0.5345256879809313 0.01097958761965856 0.0026551230577751992 0.0017896879464387894\n",
      "119 0.5354290980030783 0.011280346661806107 0.0029017476364970205 0.0022085970267653465\n",
      "120 0.5340498379664496 0.01104542356915772 0.0026806926587596537 0.0021931152790784837\n",
      "121 0.5340386080206372 0.011223485576920211 0.0029401462292298675 0.0020186522975564\n",
      "122 0.5345716089941561 0.010784098005387932 0.002704456553328782 0.0020121123641729357\n",
      "123 0.5345305780065246 0.010710761824157089 0.0025384015869349243 0.0018926090560853481\n",
      "124 0.534691498032771 0.010831268271431327 0.0025767007376998664 0.002002311032265425\n",
      "125 0.534164488024544 0.01125016959849745 0.0027557656867429614 0.0017949434928596019\n",
      "126 0.5342578680138104 0.010859137400984764 0.0024879745789803563 0.001963348127901554\n",
      "127 0.5341274879756384 0.010791345150209963 0.002473869128152728 0.0019162020646035672\n",
      "128 0.5347518980270252 0.010735557531006634 0.0024463724344968795 0.0018787126056849957\n",
      "129 0.5354196379776113 0.010837851208634675 0.0025570465018972754 0.0019119374454021453\n",
      "130 0.5353587089921348 0.010781112941913307 0.0025263844756409524 0.0020930356346070766\n",
      "131 0.5354936679941602 0.01093917724210769 0.0025668885093182324 0.0018176414072513581\n",
      "132 0.5351296680164523 0.010750826622825116 0.0024076002184301613 0.001707671955227852\n",
      "133 0.5342723380308598 0.010765273123979568 0.002437055518385023 0.0019317721948027612\n",
      "134 0.5352370080072433 0.010800277115777135 0.0024974635103717445 0.00205380879342556\n",
      "135 0.5344178180093877 0.01084084587637335 0.002543165162205696 0.0017027447000145912\n",
      "136 0.5348049380118027 0.010726349311880767 0.002519827731885016 0.0023017777130007744\n",
      "137 0.5348402980016544 0.011107638420071453 0.002847971022129059 0.0017445700243115426\n",
      "138 0.5347780289594084 0.01128659164533019 0.0027697884710505606 0.0017062431201338769\n",
      "139 0.5351944579742849 0.011082072043791413 0.0027667614165693523 0.002566027641296387\n",
      "140 0.5339542779838666 0.011328805063385516 0.002899954142048955 0.0016009172424674035\n",
      "141 0.5340389180346392 0.010903597576543689 0.0024530959548428656 0.0018342122435569762\n",
      "142 0.535100418026559 0.010762944293674082 0.002459873375482857 0.001980002038180828\n",
      "143 0.5344347080099396 0.010942769644316286 0.002599202282726765 0.002502369973808527\n",
      "144 0.5344391780090518 0.0107536178547889 0.0025746062863618137 0.0017219876870512962\n",
      "145 0.5347832279512659 0.010782578377984464 0.0024939661379903555 0.00184644078835845\n",
      "146 0.5349277689820156 0.010703772597480565 0.0023428605287335812 0.0019366550259292126\n",
      "147 0.5358118980075233 0.01086812827270478 0.0025162288453429936 0.001757310889661312\n",
      "148 0.5342077479581349 0.01094255328644067 0.0025287381606176494 0.0018149927258491517\n",
      "149 0.534579777973704 0.010804162186104804 0.0024745712871663272 0.002125092875212431\n",
      "150 0.5353532379958779 0.010771634289994836 0.0025258134584873913 0.0018742810934782028\n",
      "151 0.5364711980218999 0.010832268977537751 0.002561125298961997 0.0021519018337130547\n",
      "152 0.5355236480245367 0.010819473303854465 0.0025667497888207436 0.0020079946145415304\n",
      "153 0.5350188180455007 0.010782808414660394 0.002395177399739623 0.0018151593394577503\n",
      "154 0.5350791480159387 0.011051450041122735 0.002709150325972587 0.0017682063393294812\n",
      "155 0.535360848007258 0.01079179672524333 0.0024527641013264655 0.002077403571456671\n",
      "156 0.535583067976404 0.010881580063141882 0.0025604112772271035 0.0024622060358524323\n",
      "157 0.5343651290168054 0.011161037837155163 0.0029241655953228473 0.002340485155582428\n",
      "158 0.5344761979649775 0.011142575822304934 0.002787855570204556 0.0021064434200525283\n",
      "159 0.5353471179842018 0.010778230265714228 0.0024771311203949154 0.0019878486171364786\n",
      "160 0.535583937948104 0.010895843093749136 0.0026081153424456715 0.0020670566707849504\n",
      "161 0.5344035479938611 0.01099985180189833 0.002772338793147355 0.0026681859977543353\n",
      "162 0.5343415579991415 0.010783913952764124 0.002600295841693878 0.002027548849582672\n",
      "163 0.5346748580341227 0.010855950647965074 0.0025684561114758253 0.0019133763387799263\n",
      "164 0.534573137992993 0.010784451151266694 0.002471988776233047 0.0018984897993505\n",
      "165 0.5348455589846708 0.010921588924247772 0.002580065722577274 0.001860960852354765\n",
      "166 0.5344380380120128 0.010807288635987788 0.0024380091810598968 0.0016680863685905934\n",
      "167 0.5347542180097662 0.010776060749776661 0.0024278438882902264 0.0016894045285880566\n",
      "168 0.5346225280081853 0.010723696497734636 0.0023977417964488267 0.0016935720574110746\n",
      "169 0.5344227379537188 0.010752512724138796 0.0024169642478227616 0.002149582002311945\n",
      "170 0.5348623979953118 0.010748887259978801 0.002462952770292759 0.0016777158249169587\n",
      "171 0.5347087879781611 0.01090643834322691 0.0025243649492040277 0.0016683397348970174\n",
      "172 0.5347361280000769 0.011113608838059008 0.0027005179435946047 0.0017604850232601165\n",
      "173 0.5342795989708975 0.010830689803697169 0.002513240836560726 0.0026979345828294756\n",
      "174 0.5356095979805104 0.010929287876933813 0.002745961188338697 0.0018045376986265183\n",
      "175 0.534816917963326 0.01078792632324621 0.0024485862348228695 0.0018328645266592503\n",
      "176 0.5353805779595859 0.010780242504552007 0.002427293825894594 0.0016642860602587462\n",
      "177 0.5346675179898739 0.010809535742737353 0.002464841795153916 0.0015885204076766968\n",
      "178 0.534819817985408 0.01092847716063261 0.002507476671598852 0.0021737887524068355\n",
      "179 0.5349548379890621 0.010907470248639584 0.0025695588206872343 0.002246670611202717\n",
      "180 0.535906957986299 0.010925420501735061 0.0026662801625207066 0.0017650424502789973\n",
      "181 0.5354088179883547 0.010787028237245977 0.002410786342807114 0.0016800879500806331\n",
      "182 0.5358571379911155 0.010804929828736931 0.002463042247109115 0.0020712513476610185\n",
      "183 0.5353481189813465 0.011063017882406712 0.002546098199672997 0.0016615399159491061\n",
      "184 0.5347778480499983 0.010849539452465251 0.0025378820719197392 0.0018111139535903931\n",
      "185 0.5347691480419599 0.010848366771824658 0.0025056246435269714 0.002171762194484472\n",
      "186 0.5343055980047211 0.011044163955375552 0.002871996001340449 0.0016431830357760192\n",
      "187 0.5347888979595155 0.010749596171081066 0.0026357674039900305 0.001888447068631649\n",
      "188 0.5353985279798508 0.01084254018496722 0.0024334068177267907 0.0016619240399450065\n",
      "189 0.5351216879789717 0.010895776620600373 0.0025018991669639944 0.0020985109731554985\n",
      "190 0.5345665880013257 0.010729764064308256 0.00239716200158 0.00180470310151577\n",
      "191 0.5347348779905587 0.010761557728983462 0.0023387480527162553 0.0016009065322577954\n",
      "192 0.5352812290075235 0.010829861741513014 0.0024894428672268985 0.001861760765314102\n",
      "193 0.534813767997548 0.010970653733238578 0.00261577470228076 0.00202938811853528\n",
      "194 0.5344356680288911 0.010798725066706538 0.002475573355332017 0.0020222585648298264\n",
      "195 0.53528584796004 0.010792605229653418 0.0024276791140437127 0.0016736817546188832\n",
      "196 0.5348621680168435 0.010870702972169966 0.002454928425140679 0.0017678730189800262\n",
      "197 0.5349005880416371 0.010853585205040872 0.0025197746697813274 0.0016922937706112863\n",
      "198 0.5348073079949245 0.010857902525458485 0.002428491599857807 0.0018634838983416558\n",
      "199 0.534548127965536 0.010801538242958486 0.002447279589250684 0.0021516655571758745\n",
      "200 0.5350832890253514 0.0108463991782628 0.0027288685087114573 0.001776942890137434\n",
      "201 0.5349151080008596 0.010890464822296053 0.002483284892514348 0.0017449779435992241\n",
      "202 0.5345731179695576 0.010726271197199821 0.0025344538735225797 0.0021649859845638275\n",
      "203 0.5346519079757854 0.011009193607605994 0.002696589450351894 0.0018460836261510849\n",
      "204 0.5349449680070393 0.010700374783482403 0.0023039986845105886 0.0016754576936364174\n",
      "205 0.5352911879890598 0.010674630291759968 0.00229103690944612 0.0017330855131149291\n",
      "206 0.5348064579884522 0.01070627587614581 0.002290704764891416 0.0015648516826331615\n",
      "207 0.5347703179577366 0.01073522336082533 0.0023227109108120204 0.0016260500531643629\n",
      "208 0.5353233080240898 0.010732590279076248 0.002286108513362706 0.0015841128304600715\n",
      "209 0.5356297579710372 0.010728484950959682 0.0023301975801587106 0.0017886913381516933\n",
      "210 0.5347708190092817 0.010767121391836554 0.0023639267543330788 0.0017699363641440868\n",
      "211 0.5349571779952385 0.010692598298192024 0.0023841577698476613 0.0017552045173943043\n",
      "212 0.5352882479783148 0.010739365185145289 0.0023293759441003203 0.001811212208122015\n",
      "213 0.5362165180267766 0.010669622395653278 0.0022827939712442456 0.0016328536905348301\n",
      "214 0.5351868980214931 0.010692033625673503 0.0022743921959772705 0.0016037637367844581\n",
      "215 0.5346801279811189 0.01068791642319411 0.0023231620201840998 0.001707530254498124\n",
      "216 0.536541657987982 0.010652939090505242 0.0022540232632309197 0.0016184343025088311\n",
      "217 0.5362012379919179 0.010662314132787287 0.0022636625450104474 0.001739775203168392\n",
      "218 0.5361122679896653 0.010755546856671572 0.0022692786995321514 0.0015721235424280167\n",
      "219 0.5358298679930158 0.010935899976175278 0.002543409331701696 0.001847838144749403\n",
      "220 0.5360670680529438 0.010774450725875795 0.0023646021727472544 0.0017569707706570625\n",
      "221 0.53600462799659 0.010705219872761518 0.002305015968158841 0.0017519413493573665\n",
      "222 0.5371538879699074 0.010652753000613302 0.0022691322257742284 0.0016219713725149632\n",
      "223 0.5363817979814485 0.010773550427984446 0.0023705609375610946 0.0019537756219506265\n",
      "224 0.5357948680175468 0.01099832751788199 0.002459847228601575 0.0016799404285848141\n",
      "225 0.5358059080317616 0.01079089439008385 0.0023978916928172112 0.0019761256873607634\n",
      "226 0.535405168950092 0.010727407818194479 0.0023262307979166506 0.0015868159011006356\n",
      "227 0.5358488780329935 0.010716991731896996 0.0023285570554435254 0.0018317503854632378\n",
      "228 0.536290937976446 0.010725040512625128 0.0023041263106279076 0.0016359183471649885\n",
      "229 0.5353426680085249 0.010653140721842647 0.0022901829797774553 0.0017699082382023335\n",
      "230 0.5360905280103907 0.0107536535942927 0.0023385815089568495 0.0017193609848618508\n",
      "231 0.5355469979695044 0.010742186161223799 0.002370955410879105 0.001981088146567345\n",
      "232 0.5357660579611547 0.010748499946203083 0.002404310624115169 0.0016452489886432885\n",
      "233 0.536151698033791 0.010698918835259974 0.002327376208268106 0.0017775986343622208\n",
      "234 0.5355656680185348 0.010674198623746634 0.0023426095256581904 0.0017221907619386911\n",
      "235 0.5369025280233473 0.010810538835357875 0.0023121000500395893 0.001541875023394823\n",
      "236 0.5363143379800022 0.010783332050777972 0.0023768371902406216 0.001859764661639929\n",
      "237 0.5366964780259877 0.010735843912698328 0.0024263392668217422 0.0018460502848029136\n",
      "238 0.5359630879829638 0.01078727759886533 0.002386016515083611 0.0016612062696367501\n",
      "239 0.5357389179989696 0.010708758200053126 0.002316985349170864 0.0016654371283948421\n",
      "240 0.5360715479473583 0.010664813627954572 0.0022673237835988402 0.0016118361614644527\n",
      "241 0.5373434179928154 0.010659624123945832 0.002253195771481842 0.0017318936064839364\n",
      "242 0.5358827780000865 0.010736761905718595 0.0023134917952120303 0.0017902536317706108\n",
      "243 0.5355432579526678 0.010689205024391413 0.0022754781413823364 0.0015852360986173153\n",
      "244 0.5367713380255736 0.010747672116849571 0.002351025608368218 0.0017052315175533294\n",
      "245 0.5368586180265993 0.01081274205353111 0.002404102240689099 0.0016997785307466984\n",
      "246 0.5370182280312292 0.010676011734176427 0.0023548130877315996 0.0017797204665839673\n",
      "247 0.5358481479925103 0.010826538258697838 0.002321305056102574 0.0015542341861873865\n",
      "248 0.5357929979800247 0.010678697726689279 0.00225183900911361 0.001680835336446762\n",
      "249 0.5357276679715142 0.010716070188209414 0.0023411728208884595 0.0019931930117309095\n",
      "250 0.5364686980028637 0.010827557183802128 0.002416257676668465 0.0015377308707684278\n",
      "251 0.536014729004819 0.010763791447971016 0.0023201187141239643 0.00169529072009027\n",
      "252 0.5359265980077907 0.010697843972593546 0.0023486915975809096 0.0018068218603730201\n",
      "253 0.5357956279767677 0.010676205507479608 0.0023046203423291445 0.001623173663392663\n",
      "254 0.5355614779982716 0.010628499323502183 0.0022633499233052135 0.0018320444971323014\n",
      "255 0.5358087179483846 0.010811009793542325 0.0024494565557688476 0.0018867546692490577\n",
      "256 0.5361419680411927 0.01089108403539285 0.002473968081176281 0.002016082964837551\n",
      "257 0.5367442980059423 0.010721946018747985 0.0024537314428016545 0.0018290989100933075\n",
      "258 0.5365585779654793 0.010674519173335284 0.0023473053239285945 0.0016112299170345068\n",
      "259 0.5357627280172892 0.010642315784934908 0.0022577366908080876 0.0019113139249384403\n",
      "260 0.5364102580351755 0.01089276501443237 0.0024819241603836416 0.0016807703301310538\n",
      "261 0.5374836780247279 0.010754457674920559 0.002316809049807489 0.0016400194261223077\n",
      "262 0.5360206280020066 0.010674062592443079 0.002246584789827466 0.0016730456613004208\n",
      "263 0.5364683180232532 0.010689048387575895 0.002297415933571756 0.002130077965557575\n",
      "264 0.5374012080137618 0.010678626247681677 0.0023527928977273406 0.0017885996960103512\n",
      "265 0.5365487179951742 0.010703195410314947 0.0022856695693917574 0.0015933615621179342\n",
      "266 0.536566078022588 0.010747960419394076 0.002326944377273321 0.001896029617637396\n",
      "267 0.538018477964215 0.01071686390787363 0.0023357619415037335 0.0016330271027982235\n",
      "268 0.5371260879910551 0.010647385031916201 0.002249740133993328 0.0016789393499493598\n",
      "269 0.5369985779980198 0.010872819228097796 0.0023715597577393054 0.0016321644186973571\n",
      "270 0.5360357180470601 0.010744300205260515 0.002370716666337103 0.001873105764389038\n",
      "271 0.5373447380261496 0.010741214908193797 0.002258143201470375 0.001639372855424881\n",
      "272 0.5373585380148143 0.010685570712666959 0.0023172279819846154 0.00185456657782197\n",
      "273 0.5362316279788502 0.010754582879599184 0.0023782440694049 0.001728117698803544\n",
      "274 0.536744947952684 0.010796368645969778 0.0023603637237101793 0.0015781523659825325\n",
      "275 0.5358143780031241 0.010951769421808422 0.002498066681437194 0.0020086919888854028\n",
      "276 0.5364794380147941 0.010783573146909475 0.0024652820779010654 0.0017134458757936954\n",
      "277 0.5366167979664169 0.010787796578370035 0.0023860373767092824 0.0015746850986033678\n",
      "278 0.5370241680066101 0.010786618862766773 0.002414599002804607 0.0017098018899559975\n",
      "279 0.5360746380174533 0.010697592806536704 0.0022804154548794033 0.001588691771030426\n",
      "280 0.5361061380244792 0.010720688034780324 0.0022883434547111394 0.0015657832846045494\n",
      "281 0.5358702979865484 0.01067794457776472 0.002234526933170855 0.0016510872170329093\n",
      "282 0.538580508029554 0.010663610300980508 0.0022681669797748327 0.0017057126853615046\n",
      "283 0.5372868980048224 0.010652659693732858 0.002248907752800733 0.0018545681610703467\n",
      "284 0.5370993780088611 0.010713471216149628 0.002290104958228767 0.0015177406370639802\n",
      "285 0.536284368019551 0.010706000088248402 0.002232103399001062 0.0017075641080737114\n",
      "286 0.5371098479954526 0.010733189061284065 0.002325770864263177 0.0018269087187945843\n",
      "287 0.5371275779907592 0.010763543541543186 0.002325079869478941 0.0015653258189558983\n",
      "288 0.5369794380385429 0.010684690903872252 0.0022513374919071794 0.0016266913618892432\n",
      "289 0.5370156080462039 0.010650205367710441 0.0022224317071959376 0.001570509187877178\n",
      "290 0.5363177180406637 0.010707010165788233 0.002241201768629253 0.001558036357164383\n",
      "291 0.5357984880101867 0.01067622029222548 0.002263561834115535 0.0016854687593877316\n",
      "292 0.5357936379732564 0.010695151053369045 0.002244323072955012 0.0015507970936596393\n",
      "293 0.5362253880011849 0.01067917444743216 0.002273054723627865 0.001807983499020338\n",
      "294 0.5359334779786877 0.010704865620937198 0.002260895585641265 0.001651057368144393\n",
      "295 0.5363196679973044 0.010712086397688836 0.002278566523455083 0.0016005140263587237\n",
      "296 0.5360047480207868 0.010835423890966922 0.002310502657201141 0.0015814209822565318\n",
      "297 0.5359658779925667 0.010731359536293894 0.0023463925113901495 0.0018795361742377282\n",
      "298 0.5356749880011193 0.010732861934229732 0.0023002991219982505 0.0015991572756320239\n",
      "299 0.5364591479883529 0.010692744050174952 0.0022186637157574295 0.001653722207993269\n",
      "300 0.5364269380224869 0.010706637636758387 0.002215553307905793 0.0015449167229235172\n",
      "301 0.5361071579973213 0.010681310843210667 0.0022271880879998207 0.001646636938676238\n",
      "302 0.5358195379958488 0.0106712551205419 0.0022250111447647213 0.0016183162108063698\n",
      "303 0.5359013989800587 0.010693650401663035 0.0022323486395180226 0.0015974530950188636\n",
      "304 0.5357677980209701 0.010665319045074284 0.0022008090745657682 0.001630899729207158\n",
      "305 0.5362892179982737 0.010682663880288601 0.0022121344693005083 0.0015618841629475356\n",
      "306 0.5361649080296047 0.010673009732272476 0.002217049337923527 0.0016163420863449574\n",
      "307 0.5362275579827838 0.010677393060177565 0.0022106565767899154 0.0016224761959165335\n",
      "308 0.5358350580208935 0.010665223118849099 0.0022309677209705115 0.0016120566986501216\n",
      "309 0.5361558080185205 0.010677752085030079 0.002216608531307429 0.0015720768831670284\n",
      "310 0.5365213080076501 0.010653996490873396 0.0022135926643386482 0.0016132073942571878\n",
      "311 0.5364484780002385 0.010774734430015087 0.002262794564012438 0.0017024822533130645\n",
      "312 0.536467298050411 0.010663098481018096 0.0022613486275076867 0.001594285061582923\n",
      "313 0.5361948880017735 0.010643450368661433 0.0022777302423492075 0.0016326297074556351\n",
      "314 0.5359687180025503 0.010749072534963489 0.002267033129464835 0.0017275219317525626\n",
      "315 0.5363809280097485 0.010663307446520776 0.002221527951769531 0.0016467059962451459\n",
      "316 0.536613447999116 0.010648015246260911 0.0022100674454122783 0.0017279357649385928\n",
      "317 0.5364357980433851 0.010700500861275941 0.0022596745984628797 0.001606900990009308\n",
      "318 0.5363672579987906 0.010654719488229603 0.0022297660587355496 0.0015935557894408703\n",
      "319 0.5363746979855932 0.010706176806706935 0.002210723189637065 0.0016217296943068504\n",
      "320 0.5363752180128358 0.01068793551530689 0.0022230008384212852 0.0015989477746188641\n",
      "321 0.5364268780103885 0.010669045965187252 0.002260819613002241 0.001603488437831402\n",
      "322 0.5361938080168329 0.010685122746508569 0.0022487713606096802 0.001629369519650936\n",
      "323 0.5366174380178563 0.01068840135121718 0.0022040528012439607 0.0015800973400473596\n",
      "324 0.5361276679905131 0.010670694056898355 0.002238138346001506 0.0015961734112352132\n",
      "325 0.5363629880012013 0.010692458599805832 0.0022148011485114694 0.001683531468734145\n",
      "326 0.5365076279849745 0.0106837063212879 0.002225865295622498 0.001549485418945551\n",
      "327 0.5362538780318573 0.010677857208065689 0.0022056478541344405 0.0016136725898832083\n",
      "328 0.5359806980122812 0.01067858817987144 0.002228989568538964 0.0016825810074806214\n",
      "329 0.5365888679516502 0.010744541941676289 0.0023562527028843762 0.0016319905407726765\n",
      "330 0.5360806879471056 0.010750299436040223 0.0023308279924094677 0.0015302817802876234\n",
      "331 0.536098528013099 0.010690654395148158 0.0023513531545177103 0.0017148351296782493\n",
      "332 0.5356979679781944 0.010727073880843818 0.002281339326873422 0.0016352607868611812\n",
      "333 0.536118409014307 0.010742445942014456 0.002286549098789692 0.0019230562262237071\n",
      "334 0.5360316390288062 0.010750623419880867 0.002303720230702311 0.0017985558137297631\n",
      "335 0.5359264080179855 0.01065118657425046 0.0023240031441673636 0.0018618403933942317\n",
      "336 0.5357003479730338 0.010783369070850313 0.002258616080507636 0.0016437048092484475\n",
      "337 0.535612398001831 0.010645577975083143 0.0022709879791364073 0.0017335007898509503\n",
      "338 0.5361690180143341 0.010883671639021486 0.002337548090144992 0.0017148415092378855\n",
      "339 0.5363409080309793 0.01062592986272648 0.0022992513375356795 0.0016275421250611544\n",
      "340 0.536070958012715 0.010765485116280615 0.002298840880393982 0.0018158069811761378\n",
      "341 0.53548778797267 0.010672656178940088 0.002258146950043738 0.0015838576480746269\n",
      "342 0.536367278022226 0.010667447524610907 0.0023218893213197587 0.0017729762010276317\n",
      "343 0.5356258279643953 0.010763391794171184 0.002249245555140078 0.0014912584330886603\n",
      "344 0.5360963079729117 0.010663167980965227 0.0022475854493677616 0.0017378181219100952\n",
      "345 0.5359096580068581 0.010770041611976922 0.002285201638005674 0.0015590262599289417\n",
      "346 0.5358436980168335 0.010655136778950691 0.002232106984592974 0.0016756281722337008\n",
      "347 0.5354991480126046 0.010698505269829184 0.0022473013726994396 0.0016652083955705165\n",
      "348 0.5360203279997222 0.010680322244297713 0.002248881722334772 0.001566165452823043\n",
      "349 0.5358904380118474 0.010646584269125015 0.0021849320735782387 0.0017292392440140247\n",
      "350 0.5365187179995701 0.010737731587141752 0.0022193829528987406 0.001632407307624817\n",
      "351 0.5356092879665084 0.010662287066224962 0.0022390649537555873 0.0015855410601943731\n",
      "352 0.5360860979999416 0.010663559252861887 0.00223993007093668 0.0016021291259676218\n",
      "353 0.5355131489923224 0.010718712990637869 0.002252295264042914 0.0015539804939180613\n",
      "354 0.5365351579966955 0.010643332963809371 0.0022043967386707664 0.0016530361957848073\n",
      "355 0.5360159380361438 0.010684214415960014 0.002236495248507708 0.0016075383871793747\n",
      "356 0.5358259279746562 0.010794364497996867 0.002284091943874955 0.0016797568649053573\n",
      "357 0.5359426880022511 0.010696061188355088 0.002316562854684889 0.001539957383647561\n",
      "358 0.5352193880244158 0.010696582437958568 0.0022189732640981673 0.0016762844286859035\n",
      "359 0.5372885380056687 0.010675976460333914 0.002221625670790672 0.0015889426693320274\n",
      "360 0.5362176480120979 0.010695558099541813 0.002210971398744732 0.0015756138134747743\n",
      "361 0.5366216580150649 0.010673085052985698 0.0022015491966158153 0.001681884378194809\n",
      "362 0.5356945380335674 0.010667427210137248 0.002273855695966631 0.0017598280683159828\n",
      "363 0.5359619079972617 0.010713597119320184 0.0022706618765369058 0.001799226738512516\n",
      "364 0.5363622879958712 0.010782452940475196 0.0023754936875775455 0.0016102243214845657\n",
      "365 0.5352825180161744 0.01064954954199493 0.0023279412649571895 0.0016492698341608047\n",
      "366 0.5355070080258884 0.01075152267003432 0.002225968218408525 0.0015481485053896904\n",
      "367 0.5353054080042057 0.010652045777533203 0.0022258865647017956 0.0016141188330948352\n",
      "368 0.5348544679582119 0.010713443392887712 0.002219625446014106 0.0015759751200675963\n",
      "369 0.5349355180514976 0.010653986711986363 0.002204188145697117 0.0015764190815389157\n",
      "370 0.535231098998338 0.010634921898599714 0.00220821937546134 0.0017619813792407514\n",
      "371 0.5359984780079685 0.010782431403640658 0.0022956174798309803 0.001513763703405857\n",
      "372 0.5347588180447929 0.010687595407944173 0.0022156592342071233 0.001691762125119567\n",
      "373 0.5352771279867738 0.010687098663765937 0.00223337362986058 0.0015507890842854978\n",
      "374 0.53452973801177 0.010734642972238362 0.0023122679209336638 0.0018476190976798534\n",
      "375 0.5357988780015148 0.01070889289258048 0.0022752505028620362 0.0015514306724071503\n",
      "376 0.534687968029175 0.0106364541570656 0.002208278118632734 0.0016726746689528227\n",
      "377 0.5343598579638638 0.0107712343451567 0.002247392968274653 0.0016530978493392468\n",
      "378 0.534563348046504 0.010621003108099103 0.002195171662606299 0.0015675028786063193\n",
      "379 0.535471449024044 0.01066856668330729 0.0022031938075087964 0.0016296765301376582\n",
      "380 0.5355206080130301 0.010694662341848016 0.002236206934321672 0.0015810157172381877\n",
      "381 0.5344107279670425 0.010713898635003716 0.0022501721046864986 0.0017862604930996896\n",
      "382 0.5346231079893187 0.010712825343944132 0.0023460060823708774 0.0017558948136866092\n",
      "383 0.5344581480021589 0.010686359484679997 0.0022386542055755853 0.001548946090042591\n",
      "384 0.5342255380237475 0.010685056855436414 0.0022617683745920656 0.0017194999381899834\n",
      "385 0.5354917279910296 0.010733812698163092 0.0022204601438716052 0.0015553529839962721\n",
      "386 0.5355818680254743 0.010651840944774449 0.0022451830329373477 0.0016789688728749753\n",
      "387 0.5350476079620421 0.010722132574301213 0.002225194126367569 0.0017500569112598896\n",
      "388 0.5342132790246978 0.01070153311593458 0.0022785504115745425 0.0016599135007709265\n",
      "389 0.5345067479647696 0.010668210394214839 0.0022591329645365475 0.0015901642851531505\n",
      "390 0.5350893680006266 0.0107279903604649 0.0022276710136793555 0.001626146864145994\n",
      "391 0.5354732680134475 0.01067563972901553 0.002283005719073117 0.0016498311422765255\n",
      "392 0.5340742180123925 0.010675666970200837 0.002241225296165794 0.0015923149418085813\n",
      "393 0.5343666879925877 0.010645643924362957 0.0021868595271371306 0.001622499478980899\n",
      "394 0.5344600980170071 0.010662483575288206 0.0022052388871088625 0.0016036065760999918\n",
      "395 0.5349660380161367 0.010708163667004555 0.0021981363533996046 0.001533091813325882\n",
      "396 0.5347048789844848 0.010661818028893322 0.0022284855134785176 0.0016014487016946077\n",
      "397 0.5348301280173473 0.01085085142403841 0.0023282543173991145 0.001597263989970088\n",
      "398 0.5346445479663089 0.010658342333044857 0.0022149647353217005 0.0015428620390594006\n",
      "399 0.5357923679985106 0.010656343481969088 0.0021946395048871637 0.0016162901185452939\n",
      "400 0.5344682380091399 0.010680465726181865 0.002200308418832719 0.0016582051757723093\n",
      "401 0.536143897974398 0.010686208202969283 0.002189548406749964 0.0015705741476267575\n",
      "402 0.5354016979690641 0.010633841156959534 0.0022018591640517115 0.0015616999939084054\n",
      "403 0.5344645379809663 0.010673077020328492 0.002189110964536667 0.0016103249974548817\n",
      "404 0.5343569180113263 0.010705430409871042 0.0022050440311431885 0.001563901174813509\n",
      "405 0.5340339390095323 0.010643463756423444 0.002211046824231744 0.0015279036480933428\n",
      "406 0.5343504979973659 0.01066740317037329 0.00218653769697994 0.0015980353578925132\n",
      "407 0.5354654080001637 0.010682223655749112 0.0021875999751500784 0.0016142332926392556\n",
      "408 0.5347795280395076 0.010686891502700746 0.0022123622824437917 0.0016256732866168021\n",
      "409 0.5343036080012098 0.010690458759199828 0.0021883367793634535 0.0015344118233770131\n",
      "410 0.534061398007907 0.010634148900862783 0.0021995429415255785 0.001585541106760502\n",
      "411 0.5346827280009165 0.010667112364899367 0.0021842609159648417 0.0016096808016300202\n",
      "412 0.5338088289718144 0.010673571552615613 0.002195162675343454 0.0016004628036171199\n",
      "413 0.5336469880421646 0.01068677514558658 0.0022128297016024588 0.0015474427957087756\n",
      "414 0.5346003680024296 0.010728741472121328 0.0022182341665029524 0.0016669348813593387\n",
      "415 0.5346742080291733 0.010666004149243236 0.002201578393578529 0.00153793771751225\n",
      "416 0.5340652180020697 0.010655152436811477 0.0022029820363968612 0.0015555530786514281\n",
      "417 0.5338816180010326 0.010729654022725299 0.002233006269671023 0.0016328501049429178\n",
      "418 0.5343076979625039 0.010678878228645772 0.002199848566669971 0.0015699215233325957\n",
      "419 0.5341022689826787 0.010647798248101026 0.002188656572252512 0.0015749855432659388\n",
      "420 0.5336315180175006 0.010718485340476036 0.0022176357451826334 0.0016226832289248706\n",
      "421 0.5345880480017513 0.010678420367185026 0.0022395547945052384 0.001557313371449709\n",
      "422 0.5346070380182937 0.010698904166929424 0.002215914777480066 0.0016462858766317368\n",
      "423 0.5351907379808836 0.010653935954906046 0.002200583810918033 0.001600982341915369\n",
      "424 0.5345648580114357 0.010692086070775986 0.002190321497619152 0.001554107340052724\n",
      "425 0.5352053479873575 0.010620804561767727 0.002216389519162476 0.0015826543793082237\n",
      "426 0.5357545089791529 0.01069327414734289 0.002185981476213783 0.001626314828172326\n",
      "427 0.5348652090178803 0.01067661278648302 0.0021933758165687323 0.0015978951007127762\n",
      "428 0.5344496080069803 0.01064533629687503 0.002191857877187431 0.0015767865814268589\n",
      "429 0.5358000679989345 0.010706164874136448 0.002233487321063876 0.0016654581297188998\n",
      "430 0.5352774280472659 0.010685293702408671 0.0022154808044433593 0.001529934024438262\n",
      "431 0.53520948800724 0.010682081512641162 0.002198183652944863 0.0016444376204162836\n",
      "432 0.5337606879766099 0.010699428268708289 0.0022046837955713274 0.0015715606044977904\n",
      "433 0.5341288179624826 0.01065547892358154 0.0021917634876444936 0.0015376133844256401\n",
      "434 0.5345931780175306 0.010667824652045965 0.0021940796403214337 0.0016124548390507698\n",
      "435 0.5339771389844827 0.01069106551585719 0.002203704684507102 0.0015653211623430253\n",
      "436 0.5338807979715057 0.010654359357431531 0.0021944375010207297 0.0015998353715986013\n",
      "437 0.5363491179887205 0.010742438840679824 0.002225201181136072 0.001578749716281891\n",
      "438 0.5341700079734437 0.010630403412505984 0.002223597397096455 0.0015314170625060797\n",
      "439 0.5345079980324954 0.01070702017750591 0.002214035554789007 0.0016141748521476984\n",
      "440 0.5344899379997514 0.010699442296754569 0.002206959156319499 0.001595704909414053\n",
      "441 0.5342680080211721 0.010658118932042271 0.002244295063428581 0.0015562326647341252\n",
      "442 0.5344462590292096 0.010697879828512669 0.0022158717503771184 0.0016424554865807294\n",
      "443 0.5334917180007324 0.010661887296009809 0.002205520821735263 0.0015675308648496866\n",
      "444 0.5347936080070212 0.010671223746612668 0.002187323337420821 0.0015861094929277898\n",
      "445 0.5343207480036654 0.010665018751751631 0.0021856230916455386 0.0016008302569389342\n",
      "446 0.5341462779906578 0.010667121154256165 0.002198237436823547 0.001562208216637373\n",
      "447 0.5344320780131966 0.010694515134673566 0.0021960662910714746 0.001568766124546528\n",
      "448 0.5338709790376015 0.01064596074866131 0.0021872348035685716 0.0015863658860325812\n",
      "449 0.5347940380452201 0.010704724467359483 0.0021964595885947346 0.0016155810095369815\n",
      "450 0.5336821179953404 0.010657842387445271 0.0022036690497770905 0.0015754055231809617\n",
      "451 0.5350497280014679 0.01066329120658338 0.0021782486466690896 0.001568041881546378\n",
      "452 0.533972438017372 0.010655258083716035 0.0021883534034714104 0.0015708735678344964\n",
      "453 0.5339951380155981 0.010705129418056458 0.0022057578316889703 0.0015632526017725467\n",
      "454 0.5336309680133127 0.010676210629753768 0.002196244732476771 0.0015917133539915084\n",
      "455 0.5349273689789698 0.010691804869566113 0.002198471431620419 0.001544960681349039\n",
      "456 0.5347460779594257 0.010647522460203618 0.002193972852546722 0.0015870328526943922\n",
      "457 0.5343348580063321 0.010661660111509264 0.002180198975838721 0.0015890917740762234\n",
      "458 0.5343101780163124 0.010682089487090707 0.0021823919960297644 0.001608822215348482\n",
      "459 0.5338012279826216 0.010629209806211293 0.0021882079308852553 0.001551729766651988\n",
      "460 0.5353776679839939 0.010690248396713287 0.002206785697489977 0.0015985602047294378\n",
      "461 0.5339732180000283 0.010653219767846167 0.0022028550622053443 0.0015951426699757575\n",
      "462 0.5339382190140896 0.010696273879148066 0.0021921315928921103 0.0015561949927359819\n",
      "463 0.5339371680165641 0.01066883746534586 0.0021842532441951334 0.0016074592713266611\n",
      "464 0.5343456779955886 0.010694090742617846 0.0021900918567553163 0.0015773105435073376\n",
      "465 0.5336487980093807 0.010655157268047333 0.0022078823065385224 0.0015231345780193806\n",
      "466 0.5340733780176379 0.010679038241505623 0.0021958344150334597 0.0016293019521981477\n",
      "467 0.5342003179830499 0.010675744211766869 0.0021917545585893095 0.0015351235400885344\n",
      "468 0.5337737089721486 0.010675917961634696 0.002187889616470784 0.001569757144898176\n",
      "469 0.5339427679427899 0.010684775363188237 0.0021848806994967163 0.0015948747750371695\n",
      "470 0.5343075179844163 0.010659633902832866 0.0021870284457691015 0.0015515233855694533\n",
      "471 0.5343486979836598 0.010665529815014452 0.002195860235951841 0.0016366150230169296\n",
      "472 0.5336632480029948 0.010662006039638072 0.0021875147707760335 0.001577740954235196\n",
      "473 0.5344507980044 0.010705810389481485 0.0022030169609934093 0.0015684018842875957\n",
      "474 0.5347606689902022 0.010645522736012936 0.002253879571799189 0.0016046172007918359\n",
      "475 0.5334817580296658 0.01074477331712842 0.0022215874632820487 0.0015512780286371709\n",
      "476 0.5336563179735094 0.01064132060855627 0.002204210148192942 0.0015399863477796316\n",
      "477 0.5340174980228767 0.01065219915471971 0.0021782804746180774 0.0016995161306113005\n",
      "478 0.5345670779934153 0.010725675732828677 0.002209648967254907 0.0015787940938025713\n",
      "479 0.5335889779962599 0.010640627646353096 0.0021903059910982846 0.0015715794637799264\n",
      "480 0.533985999005381 0.01073551649460569 0.0022142685367725788 0.0016569721512496472\n",
      "481 0.5340748680173419 0.01064789085648954 0.002199425012804568 0.0015149466693401338\n",
      "482 0.5338271080399863 0.010712980525568128 0.002214579307474196 0.0016289809718728065\n",
      "483 0.5338412580313161 0.010657608101610094 0.0021795577835291625 0.001552181039005518\n",
      "484 0.5340506779612042 0.010647589107975364 0.0021988441701978447 0.0015348371118307114\n",
      "485 0.5338366380310617 0.010727702989242971 0.0022053596563637256 0.0016732933931052686\n",
      "486 0.5339841290260665 0.010622203291859478 0.0022127921809442343 0.0015030276961624622\n",
      "487 0.5342089479672723 0.010735329473391175 0.0022443740628659723 0.0015643956139683723\n",
      "488 0.5342034979839809 0.010637761210091412 0.0021811118349432945 0.0015923222992569209\n",
      "489 0.5336017379886471 0.010654393525328487 0.0021818122593685986 0.0015850052703171968\n",
      "490 0.5342564180027694 0.010717230150476098 0.0022142042173072694 0.001620554830878973\n",
      "491 0.5340966879739426 0.010635759739670902 0.002187124569900334 0.0015592258423566818\n",
      "492 0.533834918984212 0.010650032956618816 0.0021930980728939176 0.0016470476053655147\n",
      "493 0.5337226979900151 0.01075917255366221 0.0022395308362320064 0.0015371198765933513\n",
      "494 0.5342863679979928 0.01062553800875321 0.002244716475252062 0.0015563959255814551\n",
      "495 0.5340935079730116 0.010757930751424283 0.002219896693713963 0.0016682046465575696\n",
      "496 0.5339082779828459 0.010681757295969874 0.0022145498194731774 0.0015599981416016817\n",
      "497 0.5337970679975115 0.010692024196032435 0.0021880849497392775 0.0015701903030276299\n",
      "498 0.5342506790184416 0.010645648057106882 0.0021824414841830732 0.0015587188769131898\n",
      "499 0.5336489279870875 0.010692698648199439 0.002193214779254049 0.0016264882870018482\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "model = FNO3d(modes, modes, modes, width).cuda()\n",
    "\n",
    "print(count_params(model))\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda()\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x).view(batch_size, S1, S2, T)\n",
    "\n",
    "        mse = F.mse_loss(out, y, reduction='mean')\n",
    "        # mse.backward()\n",
    "\n",
    "        y = y_normalizer.decode(y)\n",
    "        out = y_normalizer.decode(out)\n",
    "        l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "        l2.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_mse += mse.item()\n",
    "        train_l2 += l2.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            out = model(x).view(batch_size, S1, S2, T)\n",
    "            out = y_normalizer.decode(out)\n",
    "            test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "\n",
    "    train_mse /= len(train_loader)\n",
    "    train_l2 /= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_mse, train_l2, test_l2)\n",
    "# torch.save(model, path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0021202925126999617\n",
      "0 0.0017275025602430105\n",
      "0 0.000620650767814368\n",
      "0 0.0026931604370474815\n",
      "0 0.0015096209244802594\n",
      "0 0.0004633042844943702\n",
      "0 0.001782383886165917\n",
      "0 0.0014457462821155787\n",
      "0 0.001625764649361372\n",
      "0 0.000654742878396064\n",
      "0 0.0013707135803997517\n",
      "0 0.003082005074247718\n",
      "0 0.0020517618395388126\n",
      "0 0.0007609151070937514\n",
      "0 0.0017164472956210375\n",
      "0 0.0024963715113699436\n",
      "0 0.0005681633483618498\n",
      "0 0.00044113019248470664\n",
      "0 0.002367568900808692\n",
      "0 0.003031523432582617\n"
     ]
    }
   ],
   "source": [
    "pred = torch.zeros(test_u.shape)\n",
    "index = 0\n",
    "# model = torch.load(\"model/friction_ep500\")\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=1, shuffle=False)\n",
    "first_output = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        test_l2 = 0\n",
    "        if index == 0:\n",
    "            first_output = y.clone()\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        out = model(x)\n",
    "        out = y_normalizer.decode(out[:,:,:,0])\n",
    "        pred[index] = out\n",
    "\n",
    "        test_l2 += myloss(out.view(1, -1), y.view(1, -1)).item()\n",
    "        print(index, test_l2)\n",
    "\n",
    "        # cp = plt.imshow(pred[index])\n",
    "        # plt.colorbar(cp)\n",
    "        # plt.show()\n",
    "\n",
    "# for i in range(18):\n",
    "#     cp = plt.imshow(abs(pred[0,:,:,i]-first_output[0,:,:,i]))\n",
    "#     plt.colorbar(cp)\n",
    "#     plt.show()\n",
    "#     print(i)\n",
    "    # cp = plt.imshow(first_output[0,:,:,i])\n",
    "    # plt.colorbar(cp)\n",
    "    # plt.show()\n",
    "\n",
    "# cp = plt.imshow(first_output[0,:,:,0])\n",
    "# plt.colorbar(cp)\n",
    "# plt.show()\n",
    "\n",
    "# cp = plt.imshow(pred[0,:,:,0])\n",
    "# plt.colorbar(cp)\n",
    "# plt.show()\n",
    "\n",
    "# cp = plt.imshow(abs(pred[0,:,:,0]-first_output[0,:,:,0]))\n",
    "# plt.colorbar(cp)\n",
    "# plt.show()\n",
    "\n",
    "# cp = plt.imshow(pred[0,:,:,9])\n",
    "# plt.colorbar(cp)\n",
    "# plt.show()\n",
    "# cp = plt.imshow(pois_output[0,:,:,0])\n",
    "# plt.colorbar(cp)\n",
    "# plt.show()\n",
    "\n",
    "# cp = plt.imshow(abs(pred[index][:,:,0] - pois_output[0,:,:,0]))\n",
    "# plt.colorbar(cp)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f8c49416430bf6f9356715c0a0173afd7466c7f6261729e3b70b73af4f7e4ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
